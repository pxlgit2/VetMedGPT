{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.corpora.wikicorpus import extract_pages,filter_wiki\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaSpider:\n",
    "    ##########################################  search funtion ##########################################\n",
    "    # search funtion, for each query, it will get 10000 results at most\n",
    "    def search_wikipedia(self,query, max_results=9999999999): # actual max limit is 10000\n",
    "        url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        results = []\n",
    "        offset = 0\n",
    "        limit = 500  # Max limit for regular users\n",
    "        \n",
    "        while len(results) < max_results:\n",
    "            #print how many results we have downloaded so far and refresh the output\n",
    "            print(f\"Found {len(results)} results of {query}\", end=\"\\r\")\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"list\": \"search\",\n",
    "                \"srsearch\": query,\n",
    "                \"srlimit\": limit,\n",
    "                \"sroffset\": offset,\n",
    "                \"utf8\": \"1\",\n",
    "                \"format\": \"json\",\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            search_results = data[\"query\"][\"search\"]\n",
    "            \n",
    "            for result in search_results:\n",
    "                title = result[\"title\"]\n",
    "                page_id = result[\"pageid\"]\n",
    "                results.append((title.replace(' ', '_'), page_id, f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"))\n",
    "\n",
    "            if \"continue\" in data:\n",
    "                offset = data[\"continue\"][\"sroffset\"]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return results[:max_results]\n",
    "    # accept a list of queries and return a list of results\n",
    "    def search_multi_wikipedia(self,queries, limit = 10):\n",
    "        res = []\n",
    "        for query in tqdm(queries):\n",
    "            res += self.search_wikipedia(query,limit)\n",
    "        return res\n",
    "    \n",
    "    def get_wikipedia_page_content(self, title):\n",
    "        if not self.check_wikipedia_page_exists(title):\n",
    "            return None\n",
    "        url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"revisions\",\n",
    "            \"rvprop\": \"content\",\n",
    "            \"format\": \"json\",\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        # print(response)\n",
    "        data = response.json()\n",
    "        # print(data)\n",
    "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "        content = page[\"revisions\"][0][\"*\"]\n",
    "        return content\n",
    "    # get multiple pages' content, max limit is 50\n",
    "    def get_multi_wikipedia_pages_content(self,titles,verbose = True):\n",
    "        url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        titles_str = \"|\".join(titles)\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"titles\": titles_str,\n",
    "            \"prop\": \"revisions\",\n",
    "            \"rvprop\": \"content\",\n",
    "            \"format\": \"json\",\n",
    "        }\n",
    "        contents = []\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        pages = data[\"query\"][\"pages\"]\n",
    "        for page_id, page in pages.items():\n",
    "            title = page[\"title\"]\n",
    "            try:\n",
    "                content = page[\"revisions\"][0][\"*\"]\n",
    "                contents.append((title, content))\n",
    "            except:\n",
    "                continue\n",
    "        return contents\n",
    "    # get hyper links from a page, not used\n",
    "    def get_wikipedia_links(self, title,pllimit=100):\n",
    "        url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"links\",\n",
    "            \"pllimit\": pllimit,  \n",
    "            \"format\": \"json\",\n",
    "        }\n",
    "        res = []\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "        links = page.get(\"links\", [])\n",
    "        for link in links:\n",
    "            title = link[\"title\"].replace(\" \", \"_\")\n",
    "            res.append(title)\n",
    "        return res\n",
    "    # check if a page exists, not used\n",
    "    def check_wikipedia_page_exists(self, title):\n",
    "        url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"titles\": title,\n",
    "            \"format\": \"json\",\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "        if \"missing\" in page:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    ##########################################  save funtion ##########################################\n",
    "    # process page's content\n",
    "    def wiki_replace(self,s):\n",
    "        s = re.sub(':*{\\|[\\s\\S]*?\\|}', '', s)\n",
    "        s = re.sub('<gallery>[\\s\\S]*?</gallery>', '', s)\n",
    "        s = re.sub('(.){{([^{}\\n]*?\\|[^{}\\n]*?)}}', '\\\\1[[\\\\2]]', s)\n",
    "        s = filter_wiki(s)\n",
    "        s = re.sub('\\* *\\n|\\'{2,}', '', s)\n",
    "        s = re.sub('\\n+', '\\n', s)\n",
    "        s = re.sub('\\n[:;]|\\n +', '\\n', s)\n",
    "        s = re.sub('\\n==', '\\n\\n==', s)\n",
    "        # s = u'【' + d[0] + u'】\\n' + s\n",
    "        return s\n",
    "    # get content and save to csv\n",
    "    def get_content_and_save(self,title_lists,search_word,save_folder = './'):\n",
    "        # create folder if not exist\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.mkdir(save_folder)\n",
    "        contents = []\n",
    "        count = 0\n",
    "        order = 1\n",
    "        for i in tqdm(range(0,len(title_lists),50)):\n",
    "            contents += self.get_multi_wikipedia_pages_content(title_lists[i:i+50])\n",
    "            count+=50\n",
    "            if count == 5000: # save every 5000 pages\n",
    "                count=0\n",
    "                file_name = search_word+'_'+str(order)+'.csv'\n",
    "                file_path = os.path.join(save_folder,file_name)\n",
    "                self._save_content(contents,file_path)\n",
    "                contents = []\n",
    "                order+=1\n",
    "        if len(contents)>0: # save the rest pages\n",
    "            file_name = search_word+'_'+str(order)+'.csv'\n",
    "            file_path = os.path.join(save_folder,file_name)\n",
    "            self._save_content(contents,file_path)\n",
    "    def _save_content(self,contents,file_path):\n",
    "        contents = pd.DataFrame(contents,columns=['title','content'])\n",
    "        contents['content'] = contents['content'].apply(self.wiki_replace)\n",
    "        contents.to_csv(file_path,index=False)\n",
    "    ##########################################  category funtion ##########################################\n",
    "    # sample website: https://en.wikipedia.org/wiki/Special:CategoryTree?target=Category%3AAnimals&mode=all&namespaces=\n",
    "    def _get_category(self, category_title):\n",
    "        url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": 'categorytree',\n",
    "            \"category\": category_title,\n",
    "            \"format\": \"json\",\n",
    "            \"options\": json.dumps({\n",
    "                \"depth\": 1,\n",
    "                \"mode\": \"all\",\n",
    "            })\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        html_content = data['categorytree']['*']\n",
    "        if html_content == '':\n",
    "            return None\n",
    "        soup = bs(html_content, 'html.parser')\n",
    "\n",
    "        child_titles = soup.find_all('a', title=True)\n",
    "        # eliminate \"Category:\" of \"Category:XXXX\"\n",
    "        category_titles = []\n",
    "        end_titles = []\n",
    "        for title in child_titles:\n",
    "            if title['title'].startswith('Category:'):\n",
    "                processed_title = title['title'].replace('Category:', '')\n",
    "                category_titles.append(processed_title)\n",
    "            else:\n",
    "                end_titles.append(title['title'])\n",
    "        return category_titles, end_titles\n",
    "########################################## broad search funtion ##########################################\n",
    "    def get_category_broad(self, category_title,depth):\n",
    "        search_list = {}\n",
    "        end_list = {}\n",
    "        search_list[0] = [category_title]\n",
    "        end_list[0] = []\n",
    "        for i in range(1,depth+1):\n",
    "            search_list[i] = []\n",
    "            end_list[i] = []\n",
    "            bar = tqdm(search_list[i-1],desc='depth '+str(i))\n",
    "            for title in bar:\n",
    "                category_titles, end_titles = self._get_category(title)\n",
    "                search_list[i].extend(category_titles)\n",
    "                end_list[i].extend(end_titles)\n",
    "                bar.set_postfix_str(f\" end_list depth{i}:{len(end_list[i])}\")\n",
    "        combine_list = []\n",
    "        for i in range(depth+1):\n",
    "            combine_list.extend(end_list[i])\n",
    "        combine_list = self.clean_list(combine_list)\n",
    "        return search_list,end_list, combine_list  \n",
    "    def continue_search_category(self,search_dict,end_dict,depth):\n",
    "        pre_depth = max(search_dict.keys())\n",
    "        for i in range(pre_depth+1,depth+1):\n",
    "            search_dict[i] = []\n",
    "            end_dict[i] = []\n",
    "            bar = tqdm(search_dict[i-1],desc='depth '+str(i))\n",
    "            for title in bar:\n",
    "                category_titles, end_titles = self._get_category(title)\n",
    "                search_dict[i].extend(category_titles)\n",
    "                end_dict[i].extend(end_titles)\n",
    "                bar.set_postfix_str(f\" end_list depth{i}:{len(end_dict[i])}\")\n",
    "        combine_list = []\n",
    "        for i in range(pre_depth+1,depth+1):\n",
    "            combine_list.extend(end_dict[i])\n",
    "        combine_list = self.clean_list(combine_list)\n",
    "        return search_dict, end_dict, combine_list\n",
    "    def clean_list(self,combine_list):\n",
    "        combine_list = list(set(combine_list))\n",
    "        combine_list = [title.replace(' ','_') for title in combine_list]\n",
    "        return combine_list\n",
    "########################################## deep search (save) funtion ##########################################\n",
    "    def get_category_deep(self, category_title, depth, current_depth=0, base_path='./', path_chain=''):\n",
    "        if current_depth >= depth:\n",
    "            return\n",
    "        if current_depth == 0:\n",
    "            self.deep_search_collect_title_count = 0\n",
    "            current_path = os.path.join(base_path, category_title.replace(' ', '_').replace(':', '-'))\n",
    "            if not os.path.exists(current_path):\n",
    "                os.makedirs(current_path)\n",
    "        new_path_chain = f\"{path_chain} -> {category_title}\" if path_chain else category_title\n",
    "        print(f\"total collect num {self.deep_search_collect_title_count}, Processing category chain: {new_path_chain}\".ljust(500), end=\"\\r\")  \n",
    "\n",
    "        # If at target depth, collect and save titles\n",
    "        _, end_titles = self._get_category(category_title)\n",
    "        self.deep_search_collect_title_count += len(end_titles)\n",
    "        self._create_and_save(end_titles, current_path)\n",
    "        category_titles, _ = self._get_category(category_title)\n",
    "        # create folder\n",
    "        for title in category_titles:\n",
    "            subcategory_path = os.path.join(current_path, title.replace(' ', '_').replace(':', '-'))\n",
    "            if not os.path.exists(subcategory_path):\n",
    "                os.makedirs(subcategory_path)\n",
    "        # Recurse into each subcategory\n",
    "        for subcategory_title in category_titles:\n",
    "            self.get_category_deep(subcategory_title, depth, current_depth + 1, current_path, new_path_chain)\n",
    "\n",
    "\n",
    "    def _create_and_save(self, titles, folder_path):\n",
    "        contents = []\n",
    "        for i in range(0,len(titles),50):\n",
    "            contents += self.get_multi_wikipedia_pages_content(titles[i:i+50],verbose=False)\n",
    "        # Save all contents in one CSV file\n",
    "        if contents:\n",
    "            csv_path = os.path.join(folder_path, 'contents.csv')\n",
    "            df = pd.DataFrame(contents,columns=['title','content'])\n",
    "            df.to_csv(csv_path, index=False)\n",
    "    def continue_category_search(self, base_path, target_depth):\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            current_depth = root.count(os.sep) - base_path.count(os.sep)\n",
    "            if current_depth < target_depth:\n",
    "                category_title = os.path.basename(root).replace('_', ' ').replace('-', ':')\n",
    "                print(category_title)\n",
    "                self.get_category_deep(category_title, target_depth, current_depth, os.path.dirname(root))\n",
    "ws = WikipediaSpider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search according to giving word list, for each word, search up to 10000 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['animals','animal diseases','animal anatomy','animal cognition','animal communication','animal diseases','animal migration','animal physiology','animal sexuality','animal welfare','animal rights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_at_select_path(word_list, prefix):\n",
    "    for word in word_list:\n",
    "        search_result = ws.search_wikipedia(word)\n",
    "        title_list = [i[0] for i in search_result]\n",
    "        ws.get_content_and_save(title_list, prefix+'_'+word, save_folder='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search according to category  \n",
    "Sample category tree website: https://en.wikipedia.org/wiki/Special:CategoryTree?target=Category%3AAnimals&mode=all&namespaces=\n",
    "\n",
    "search_dict save the category titles which have child categories and pages\n",
    "\n",
    "end_dict save the titles whcih are not category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search first depth\n",
    "category_title = 'Animals'\n",
    "search_dict, end_dict, combine_list = ws.get_category_broad(category_title,depth= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search 2 to 5 depth\n",
    "for i in range(2,6):\n",
    "    search_dict, end_dict, combine_list = ws.continue_search_category(search_dict, end_dict, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "depth 6: 100%|██████████| 1387/1387 [06:27<00:00,  3.58it/s,  end_list depth6:18770]\n"
     ]
    }
   ],
   "source": [
    "# if you want to continue search\n",
    "search_dict, end_dict, combine_list = ws.continue_search_category(search_dict, end_dict, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also you can search from 1 to 6 with 1 line code, but I recommend you to use above practice.\n",
    "# search_dict, end_dict, combine_list = ws.get_category_broad('Animal health',6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the dict and list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dict and list as json\n",
    "with open('category_dict.json','w') as f:\n",
    "    json.dump(search_dict,f)\n",
    "with open('category_end_dict.json','w') as f:\n",
    "    json.dump(end_dict,f)\n",
    "with open('category_list.json','w') as f:\n",
    "    json.dump(combine_list,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the dict and list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dict and list from json\n",
    "with open('category_dict.json','r') as f:\n",
    "    search_dict = json.load(f)\n",
    "with open('category_end_dict.json','r') as f:\n",
    "    end_dict = json.load(f)\n",
    "with open('category_list.json','r') as f:\n",
    "    combine_list = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search and save according to combine list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [02:54<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "ws.get_content_and_save(combine_list, word = category_title + '_category',save_folder='./data/'+category_title+'_category')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
